# -*- coding: utf-8 -*-
"""preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aMQToKuFB3kl-K23T8izKao8azwieMJJ

## Load packages

> Add blockquote
"""

#!pip install seaborn
#!pip install python-louvain


import pandas as pd
import networkx as nx
import community as community
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import random
import collections
import community as community_louvain
from networkx.algorithms.community.quality import modularity as calculate_modularity

"""## Load data

IF COLAB:

from google.colab import drive
drive.mount('/content/drive')

df_info = pd.read_csv('/content/drive/MyDrive/9606.protein.info.v12.0.txt', sep='\t')
name_map = df_info.set_index('#string_protein_id')['preferred_name'].to_dict()

df_interactions = pd.read_csv('/content/drive/MyDrive/9606.protein.links.v12.0.min400.onlyAB.tsv', sep='\t')
output_file = 'protein_link_cleaned.tsv'

### Clean data so ESPN numbers is translated to protein preferred names (same as gene IDs)
"""

### IF LOCAL: ###

# --- Setting up files --- 
info_file = '_raw/9606.protein.info.v12.0.txt'
interaction_file = '_raw/9606.protein.links.v12.0.min400.onlyAB.tsv'
output_file = 'data/protein_link_cleaned.tsv'

# --- 1. Load the info file ---
# REMOVED comment='#' so Pandas reads the header line
df_info = pd.read_csv(
    info_file,
    sep='\t'
)

# --- 2. Create the conversion map (dictionary) ---
# Now this line will work, because the column '#string_protein_id' exists
name_map = df_info.set_index('#string_protein_id')['preferred_name'].to_dict()

# --- 3. Load the interaction file ---
df_interactions = pd.read_csv(interaction_file, sep='\t')


### --- Data cleaning steps for both local and colab --- ###

# --- 4. Map the IDs to new columns ---
df_interactions['protein1'] = df_interactions['protein1'].map(name_map)
df_interactions['protein2'] = df_interactions['protein2'].map(name_map)

# --- 5. Handle missing names ---
df_interactions['protein1'] = df_interactions['protein1'].fillna(df_interactions['protein1'])
df_interactions['protein2'] = df_interactions['protein2'].fillna(df_interactions['protein2'])

# --- 6. Create the final, clean DataFrame ---
df_edges = df_interactions[['protein1', 'protein2', 'combined_score']]

# --- 7. Save the result ---
df_edges.to_csv(output_file, sep='\t', index=False)

print(f"Conversion complete! File saved to {output_file}")





"""### Load cleaned interaction file and make edges"""








#pd.display(df_edges)

"""## Descriptive statistics"""
"""
plt.figure(figsize=(10, 6))
sns.histplot(df_edges['combined_score'], bins=30, kde=True)
plt.title('Distribution of Combined Score')
plt.xlabel('Combined Score')
plt.ylabel('Frequency')
plt.show()

protein_counts = collections.Counter()
for protein_list in df_edges[['protein1', 'protein2']].values:
    protein_counts.update(protein_list)

top_proteins = pd.DataFrame(protein_counts.most_common(20), columns=['Protein', 'Count'])

plt.figure(figsize=(12, 7))
sns.barplot(x='Count', y='Protein', data=top_proteins, palette='viridis')
plt.title('Top 20 Most Frequently Interacting Proteins')
plt.xlabel('Number of Interactions')
plt.ylabel('Protein ID')
plt.show()

"""## Create Graph object"""

# The combined_score will be used as the weight/attribute of the edge
def make_graph(df):
    G = nx.from_pandas_edgelist(
    df,
    source='protein1',
    target='protein2',
    edge_attr='combined_score'
)

    # You can check the resulting graph properties
    print(f"Number of nodes: {G.number_of_nodes()}")
    print(f"Number of edges: {G.number_of_edges()}")


    return G


G = make_graph(df_edges)

"""##Plot graph"""

def plot_graph(G):
  plt.figure(figsize=(10, 8))
  pos = nx.spring_layout(G, seed=42) # A standard layout algorithm

  # Draw the nodes, edges, and labels
  nx.draw_networkx_nodes(G, pos, node_size=10, node_color='blue')
  nx.draw_networkx_edges(G, pos, width=2, alpha=0.6, edge_color='black')

  plt.title("PPI Network", fontsize=14)
  plt.axis('off')
  plt.savefig('ppi_graph_plot.png')

#plot_graph(G)

def plot_degree_distribution(G):
    """
    Calculates node degrees and plots their distribution using Seaborn.
    """
    # 1. Extract the number of interactions (node degree) for all proteins
    degrees = [degree for node, degree in G.degree()]

    # Calculate the mean degree for reference
    mean_degree = np.mean(degrees)

    plt.figure(figsize=(10, 6))

    # 2. Use Seaborn's histplot to combine the histogram and the KDE curve
    sns.histplot(degrees,
                 bins=max(degrees) if degrees else 1, # Set bins based on max degree
                 kde=True,
                 color='#2c7bb6',
                 edgecolor='black',
                 linewidth=1)

    # 3. Add context elements
    plt.axvline(mean_degree, color='red', linestyle='--',
                label=f'Mean Degree: {mean_degree:.2f}')

    plt.title('Distribution of Protein Interactions (Node Degree)', fontsize=16)
    plt.xlabel('Number of Interactions (Node Degree)', fontsize=12)
    plt.ylabel('Number of Proteins', fontsize=12)
    plt.legend()
    plt.grid(axis='y', alpha=0.5)
    plt.show()


#plot_degree_distribution(G)

"""# Filtering

### Remove sticky proteins
"""

# remove sticky proteins

# Calculate the degree of every node
degrees = dict(G.degree())

# Find the mean and standard deviation of the degrees
degree_values = np.array(list(degrees.values()))
mean_degree = np.mean(degree_values)
std_dev = np.std(degree_values)

# Define a threshold: e.g., anything > (mean + 2 * standard deviation)
# This identifies nodes that are statistical outliers in connectivity.
hub_threshold = mean_degree + (2 * std_dev)

# Identify the candidate sticky proteins (hubs)
candidate_sticky_proteins = [
    node for node, degree in degrees.items()
    if degree > hub_threshold
]

print(f"Mean Degree: {mean_degree:.2f}")
print(f"Threshold (Mean + 2*SD): {hub_threshold:.2f}")
print(f"Candidate Sticky Proteins found: {len(candidate_sticky_proteins)}")

sticky_set = set(candidate_sticky_proteins)

# 2. Filter df_edges: Keep only rows where NEITHER 'protein1' nor 'protein2' is in the sticky_set
df_no_sticky_proteins = df_edges[
    (~df_edges['protein1'].isin(sticky_set)) &
    (~df_edges['protein2'].isin(sticky_set))
]

"""### Remove edges with combined score < 700"""

# Create a new DataFrame containing only edges with combined_score >= 700
df_filtered_edges = df_no_sticky_proteins[df_no_sticky_proteins['combined_score'] >= 700]

print(len(df_no_sticky_proteins[df_no_sticky_proteins['combined_score'] >= 700]))

print(f"{round((len(df_edges) - len(df_filtered_edges))/len(df_edges)*100,1)} % of edges were removed due to low combined score")

"""### Remove proteins with only 1 interaction"""

# Combine all protein IDs from both columns
all_proteins = pd.concat([df_filtered_edges['protein1'], df_filtered_edges['protein2']])

# Count the occurrences of each protein (This gives the degree)
degree_counts = all_proteins.value_counts()

# Identify proteins with Degree 0 (No connections) and Degree 1 (Only one connection)
proteins_to_remove = degree_counts[degree_counts <= 1].index.tolist()


# Create a set for fast lookup
proteins_to_remove_set = set(proteins_to_remove)

# Filter the DataFrame to remove edges connected to these proteins
df_final_edges = df_filtered_edges[
    (~df_filtered_edges['protein1'].isin(proteins_to_remove_set)) &
    (~df_filtered_edges['protein2'].isin(proteins_to_remove_set))
]

print(f"Number of proteins with exactly 1 connection: {len(proteins_to_remove_set)}")

print(f"{round((len(df_edges) - len(df_final_edges))/len(df_edges)*100,1)} % of edges were removed during filtering")
print(f"Final number of edges: {len(df_final_edges)}")

"""## Create graph object after filtering"""

G_filtered = make_graph(df_final_edges)

"""## Plot graph after filtering"""

#plot_graph(G_filtered)

"""## Extreacting Largest Connected Component Graph (LCC)"""

def _get_lcc_graph(G):
    """Internal helper to extract the Largest Connected Component (LCC) graph."""
    if not G.number_of_nodes():
        return nx.Graph() # Return empty graph if input is empty

    components = nx.connected_components(G)
    largest_component_nodes = max(components, key=len)
    G_lcc = G.subgraph(largest_component_nodes).copy()

    print(f"Original nodes: {G.number_of_nodes()}, LCC nodes: {G_lcc.number_of_nodes()}")
    print(f"Original edges: {G.number_of_edges()}, LCC edges: {G_lcc.number_of_edges()}")

    return G_lcc

def extract_largest_component(input_data):
    """
    Builds a graph from a DataFrame or uses an existing NetworkX graph,
    identifies the Largest Connected Component (LCC), and returns the edges
    belonging only to that component as a new Pandas DataFrame AND the
    corresponding NetworkX graph.

    Args:
        input_data (pd.DataFrame or nx.Graph): The edge list DataFrame (with
                                                'protein1', 'protein2', 'combined_score')
                                                OR the already-built NetworkX graph.

    Returns:
        tuple: (pd.DataFrame, nx.Graph) where the DataFrame contains only the
               edges of the LCC, and the Graph is the LCC graph itself.
    """
    if isinstance(input_data, pd.DataFrame):
        # 1. Input is DataFrame, build the graph
        G = nx.from_pandas_edgelist(
            input_data,
            source='protein1',
            target='protein2',
            edge_attr='combined_score'
        )
    elif isinstance(input_data, nx.Graph):
        # 1. Input is already a NetworkX graph
        G = input_data
    else:
        raise TypeError("Input must be a Pandas DataFrame or a NetworkX Graph.")

    # 2. Get the LCC graph
    G_lcc = _get_lcc_graph(G)

    # 3. Convert the LCC graph back into a Pandas edge list DataFrame
    df_lcc_edges = nx.to_pandas_edgelist(
        G_lcc,
        source='protein1',
        target='protein2'
    )

    # Rename the edge attribute column back to 'combined_score'
    if 'weight' in df_lcc_edges.columns:
        df_lcc_edges.rename(columns={'weight': 'combined_score'}, inplace=True)
    elif 'combined_score' not in df_lcc_edges.columns:
        print("Warning: Edge attribute 'combined_score' not found after conversion.")

    # 4. Return both the DataFrame and the Graph
    return df_lcc_edges, G_lcc




df_final_edges, G_final = extract_largest_component(G_filtered)

#plot_degree_distribution(G_final)

"""## Plot the final graph network"""

#plot_graph(G_final)

"""### Make a final csv file for the cleaned data"""

output_csv_filename = 'data/cleaned_data.csv'
df_final_edges.to_csv(output_csv_filename, index=False)
print(f"DataFrame saved to {output_csv_filename}")